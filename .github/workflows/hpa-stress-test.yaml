name: HPA Fortio stress test for autoscaling (AKS)

on:
  workflow_dispatch:
    inputs:
      cluster_name:
        description: 'AKS Cluster name'
        required: true
        default: 'aks-np-use-dev'
      resource_group:
        description: 'Azure Resource Group that contains the AKS cluster'
        required: true
        default: 'rg-np-use-dev'
      namespace:
        description: 'Kubernetes namespace'
        required: true
        default: 'default'
      service_name:
        description: 'Target service name (host-based)'
        required: true
        default: 'nodejs-service'
        type: choice
        options:
          - nodejs-service
          - nginx-service
          - k8sgpt-service
      # path now defaults to / because you moved to host-based ingress
      path:
        description: 'Service endpoint path (use / since host-based routing)'
        required: true
        default: '/'
        type: choice
        options:
          - '/'
      qps:
        description: 'Queries per second'
        required: true
        default: '500'
      load_duration:
        description: 'Duration of stress load (e.g., 120s)'
        required: true
        default: '120s'

jobs:
  hpa-stress-test:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Azure Login
      uses: azure/login@v2
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}

    - name: Install kubectl
      uses: azure/setup-kubectl@v4
      with:
        version: 'latest'

    - name: Get AKS kubeconfig
      run: |
        az aks get-credentials \
          -g "${{ github.event.inputs.resource_group }}" \
          -n "${{ github.event.inputs.cluster_name }}" \
          --overwrite-existing

    - name: Derive app label and prepare variables
      id: derive
      shell: bash
      run: |
        set -euo pipefail
        SVC="${{ github.event.inputs.service_name }}"
        case "$SVC" in
          nodejs-service)
            echo "APP_LABEL=nodejs-app" >> $GITHUB_ENV
            HOST_PREFIX="nodejs"
            ;;
          nginx-service)
            echo "APP_LABEL=nginx" >> $GITHUB_ENV
            HOST_PREFIX="nginx"
            ;;
          k8sgpt-service)
            echo "APP_LABEL=k8sgpt" >> $GITHUB_ENV
            HOST_PREFIX="k8sgpt"
            ;;
          *)
            echo "Unknown service: $SVC" >&2
            exit 1
            ;;
        esac

        # Convert duration like 120s -> 120 for `timeout`
        LD="${{ github.event.inputs.load_duration }}"
        LD_NUM="${LD%s}"
        [[ "$LD_NUM" =~ ^[0-9]+$ ]] || { echo "Invalid load_duration format. Use e.g. 120s"; exit 1; }
        echo "LOAD_DURATION_SECS=$LD_NUM" >> $GITHUB_ENV

        # expose HOST_PREFIX for later steps
        echo "HOST_PREFIX=${HOST_PREFIX}" >> $GITHUB_ENV

    - name: Resolve ingress LoadBalancer IP (LB_IP)
      shell: bash
      run: |
        set -euo pipefail
        # Wait up to ~5 minutes for ingress svc to have an IP (adjust loops/timeouts as you need)
        for i in {1..30}; do
          LB_IP=$(kubectl -n ingress-nginx get svc ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || true)
          if [ -z "$LB_IP" ]; then
            # Some clouds use hostname instead of IP
            LB_IP=$(kubectl -n ingress-nginx get svc ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
          fi
          if [ -n "$LB_IP" ]; then
            echo "Found ingress LB: $LB_IP"
            break
          fi
          echo "Waiting for ingress LB to get an IP/hostname... ($i/30)"
          sleep 10
        done

        if [ -z "${LB_IP:-}" ]; then
          echo "ERROR: could not determine ingress load balancer IP/hostname. Inspect: kubectl -n ingress-nginx get svc ingress-nginx-controller -o wide"
          exit 1
        fi

        # Persist for other steps
        echo "LB_IP=${LB_IP}" >> $GITHUB_ENV
        echo "LB_IP=$LB_IP"

    - name: Build public target URL (nip.io)
      shell: bash
      run: |
        set -euo pipefail
        # HOST_PREFIX set in derive step
        TARGET_HOST="${HOST_PREFIX}.${LB_IP}.nip.io"
        TARGET_PATH="${{ github.event.inputs.path }}"
        # Ensure path starts with / and not double slash
        if [[ "${TARGET_PATH}" != /* ]]; then TARGET_PATH="/${TARGET_PATH}"; fi
        # Build final URL (http)
        TARGET_URL="http://${TARGET_HOST}${TARGET_PATH}"
        echo "TARGET_URL=${TARGET_URL}" >> $GITHUB_ENV
        echo "TARGET_HOST=${TARGET_HOST}" >> $GITHUB_ENV
        echo "TARGET_PATH=${TARGET_PATH}" >> $GITHUB_ENV
        echo "Built target: $TARGET_URL"

    - name: Sanity - DNS resolves to LB IP (debug)
      shell: bash
      run: |
        set -euo pipefail
        echo "Resolving ${TARGET_HOST} ..."
        nslookup "${TARGET_HOST}" || host "${TARGET_HOST}" || dig +short "${TARGET_HOST}" || true

        echo "Launching one-shot pod to curl ${TARGET_URL} (will capture logs then delete pod)..."
        POD_NAME="curlcheck-$$"
        kubectl run "${POD_NAME}" \
          --image=appropriate/curl \
          -n "${{ github.event.inputs.namespace }}" \
          --restart=Never \
          --command -- sh -c "curl -sS -I --max-time 10 '${TARGET_URL}' || true"

        # wait for pod to finish (timeout 20s)
        for i in {1..20}; do
          PHASE=$(kubectl -n "${{ github.event.inputs.namespace }}" get pod "${POD_NAME}" -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
          if [ "$PHASE" = "Succeeded" ] || [ "$PHASE" = "Failed" ]; then
            break
          fi
          sleep 1
        done

        echo "----- curl pod logs -----"
        kubectl -n "${{ github.event.inputs.namespace }}" logs "${POD_NAME}" || true
        echo "----- end logs -----"

        # cleanup
        kubectl -n "${{ github.event.inputs.namespace }}" delete pod "${POD_NAME}" --ignore-not-found

    - name: Start Fortio Load (Scale-Up Trigger)
      shell: bash
      run: |
        set -euo pipefail
        echo "Starting Fortio load for ${{ github.event.inputs.load_duration }} at ${{ github.event.inputs.qps }} QPS"
        # Run Fortio inside the cluster so it can watch HPA easily and generate load via the cluster's network
        kubectl run fortio-load \
          --image=fortio/fortio \
          -n "${{ github.event.inputs.namespace }}" \
          --restart=Never \
          --command -- \
          fortio load -qps "${{ github.event.inputs.qps }}" -t "${{ github.event.inputs.load_duration }}" "${TARGET_URL}" &
        # brief pause to ensure pod starts
        sleep 5
        kubectl get pod fortio-load -n "${{ github.event.inputs.namespace }}" || true

    - name: Watch HPA Scaling (During Load)
      shell: bash
      run: |
        echo "Watching HPA during load..."
        timeout ${LOAD_DURATION_SECS} kubectl get hpa -n "${{ github.event.inputs.namespace }}" -w || true

    - name: Describe HPA(s) (snapshot during/after load)
      shell: bash
      run: |
        echo "==== HPA Describe (all) ===="
        for h in $(kubectl get hpa -n "${{ github.event.inputs.namespace }}" -o jsonpath='{.items[*].metadata.name}'); do
          echo "--- $h ---"
          kubectl describe hpa "$h" -n "${{ github.event.inputs.namespace }}" || true
        done

    - name: Cleanup Fortio Pod
      if: always()
      run: |
        kubectl delete pod fortio-load -n "${{ github.event.inputs.namespace }}" --ignore-not-found

    - name: Wait for Scale-Down Window
      run: |
        echo "Waiting 4 minutes 30 seconds to allow scale-down start (HPA stabilizationWindowSecondsâ‰ˆ300)..."
        sleep 270

    - name: Watch HPA Scaling (Scale-Down)
      run: |
        echo "Watching HPA scale down..."
        timeout 120 kubectl get hpa -n "${{ github.event.inputs.namespace }}" -w || true

    - name: Watch Pods Scaling (post-load)
      run: |
        echo "Listing pods with app label '${APP_LABEL}' after load:"
        kubectl get pods -l "app=${APP_LABEL}" -n "${{ github.event.inputs.namespace }}" -o wide
