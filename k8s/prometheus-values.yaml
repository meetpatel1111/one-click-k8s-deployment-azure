# prometheus-values.yaml - for kube-prometheus-stack Helm chart (dev-friendly)
# Use:
# helm upgrade --install prometheus prometheus-community/kube-prometheus-stack -n default -f k8s/prometheus-values.yaml

prometheus:
  prometheusSpec:
    # Important: do not auto-scrape all ServiceMonitors; use an explicit selector
    serviceMonitorSelectorNilUsesHelmValues: false

    # Select ServiceMonitors with label release=prometheus (adjust if you used a different release name)
    serviceMonitorSelector:
      matchLabels:
        release: prometheus

    # Allow Prometheus operator to discover ServiceMonitors in these namespaces
    # (use matchNames for widest CRD compatibility)
    serviceMonitorNamespaceSelector:
      matchNames:
        - default
        # - monitoring

    # Optional: reduce retention for dev/testing (set to production value when needed)
    retention: 7d
    evaluationInterval: "15s"

    # NOTE: removed `additionalArgs` that attempted to set web.route-prefix / web.external-url.
    # Using host-based ingress (grafana.<LB>.nip.io, prometheus.<LB>.nip.io) is simpler and avoids
    # the Prometheus argument/CRD rendering issues (previously caused unknown long flag errors).

    # If you really need path-based routing, re-add `additionalArgs` here but ensure the chart
    # renders them exactly how the Prometheus CRD expects (this is error-prone).
    #
    # additionalArgs:
    #   - --web.route-prefix=/prometheus
    #   - --web.external-url=/prometheus

grafana:
  enabled: true
  adminUser: admin
  adminPassword: prom-operator

  # Serve Grafana at root when using host-based ingress.
  # Removed GF_SERVER_ROOT_URL and GF_SERVER_SERVE_FROM_SUB_PATH to avoid redirect loops.
  # If you must serve Grafana under a subpath, restore these env vars and configure ingress rewrites.
  env: {}

  ingress:
    enabled: false

  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard

  # Dev mode: persistence disabled. Enable in prod.
  persistence:
    enabled: false

kubeStateMetrics:
  enabled: true

nodeExporter:
  enabled: true

alertmanager:
  enabled: true

# Default resources â€” tweak for your cluster size
resources:
  limits:
    cpu: 500m
    memory: 1Gi
  requests:
    cpu: 100m
    memory: 256Mi
